[
  {
    "document": {
      "text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-selection.md -> CrawlResult.extracted_content"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "location": "docs/md_v2/extraction/chunking.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "TopicSegmentationChunking",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class TopicSegmentationChunking(ChunkingStrategy): def __init__(self, num_keywords=3, **kwargs): import nltk as nl self.tokenizer = nl.tokenize.TextTilingTokenizer() self.num_keywords = num_keywords def chunk(self, text: str) -> list: # Use the TextTilingTokenizer to segment the text segmented_topics = self.tokenizer.tokenize(text) return segmented_topics def extract_keywords(self, text: str) -> list: # Tokenize and remove stopwords and punctuation import nltk as nl tokens = nl.toknize.word_tokenize(text) tokens = [token.lower() for token in tokens if token not in nl.corpus.stopwords.words('english') and token not in string.punctuation] # Calculate frequency distribution freq_dist = Counter(tokens) keywords = [word for word, freq in freq_dist.most_common(self.num_keywords)] return keywords def chunk_with_topics(self, text: str) -> list: # Segment the text into topics segments = self.chunk(text) # Extract keywords for each topic segment segments_with_topics = [(segment, self.extract_keywords(segment)) for segment in segments] return segments_with_topics",
        "type": "Class",
        "relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking"
      },
      {
        "title": "ChunkingStrategy",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
        "type": "Class",
        "relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
      "location": "docs/md_v2/extraction/llm.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncWebCrawler"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "location": "docs/md_v2/advanced/magic-mode.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "magic-mode.md -> CrawlResult.markdown"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "magic-mode.md -> CrawlResult.success"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
        "traceability_granularity": "Method",
        "trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/advanced/proxy-security.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
        "traceability_granularity": "Method",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "quickstart.md -> CrawlResult.markdown"
      }
    ]
  },
  {
    "document": {
      "text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
      "location": "docs/md_v2/extraction/llm.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "location": "docs/md_v2/basic/page-interaction.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
      "location": "docs/md_v2/extraction/css.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.media"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.extracted_content"
      },
      {
        "title": "CrawlResult.fit_markdown",
        "location": "crawl4ai/models.py",
        "content": "fit_markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.fit_markdown"
      }
    ]
  },
  {
    "document": {
      "text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncWebCrawler"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "overview.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "location": "docs/md_v2/basic/simple-crawling.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.html",
        "location": "crawl4ai/models.py",
        "content": "html: str",
        "type": "Class Attribute",
        "relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.html"
      },
      {
        "title": "CrawlResult.fit_markdown",
        "location": "crawl4ai/models.py",
        "content": "fit_markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.fit_markdown"
      },
      {
        "title": "CrawlResult.cleaned_html",
        "location": "crawl4ai/models.py",
        "content": "cleaned_html: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.cleaned_html"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.links",
        "location": "crawl4ai/models.py",
        "content": "links: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.links"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.success"
      },
      {
        "title": "CrawlResult.status_code",
        "location": "crawl4ai/models.py",
        "content": "status_code: Optional[int] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.status_code"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.media"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
        "traceability_granularity": "Method",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.screenshot",
        "location": "crawl4ai/models.py",
        "content": "screenshot: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "quickstart.md -> CrawlResult.screenshot"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "quickstart.md -> CrawlResult.success"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "quickstart.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "location": "docs/md_v2/extraction/llm.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncWebCrawler"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "llm.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
        "traceability_granularity": "Method",
        "trace_chain": "llm.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
      "location": "docs/md_v2/basic/page-interaction.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
      "location": "docs/md_v2/advanced/proxy-security.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
        "traceability_granularity": "Method",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "location": "docs/md_v2/basic/page-interaction.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
        "traceability_granularity": "Method",
        "trace_chain": "overview.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncWebCrawler"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "location": "docs/md_v2/basic/simple-crawling.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
        "traceability_granularity": "Method",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "location": "docs/md_v2/extraction/css.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "css.md -> CrawlResult.success"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
        "traceability_granularity": "Method",
        "trace_chain": "css.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "css.md -> CrawlResult.extracted_content"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
        "traceability_granularity": "Method",
        "trace_chain": "overview.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncWebCrawler"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
        "traceability_granularity": "Method",
        "trace_chain": "cosine.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "location": "docs/md_v2/extraction/css.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "css.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncWebCrawler"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
        "traceability_granularity": "Method",
        "trace_chain": "css.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "location": "docs/md_v2/advanced/session-management.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.media"
      }
    ]
  },
  {
    "document": {
      "text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult.error_message",
        "location": "crawl4ai/models.py",
        "content": "error_message: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "overview.md -> CrawlResult.error_message"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "overview.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
        "traceability_granularity": "Method",
        "trace_chain": "overview.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "overview.md -> CrawlResult.success"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores Base64-encoded image data that is captured according to the documented configuration parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.screenshot",
        "location": "crawl4ai/models.py",
        "content": "screenshot: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The screenshot property in CrawlResult stores the captured webpage image as a base64-encoded string that can be decoded into a PNG file as shown in the documentation example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "browser-config.md -> CrawlResult.screenshot"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.screenshot",
        "location": "crawl4ai/models.py",
        "content": "screenshot: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "browser-config.md -> CrawlResult.screenshot"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
      "location": "docs/md_v2/advanced/session-management-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "location": "docs/md_v2/advanced/session-management-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      }
    ]
  },
  {
    "document": {
      "text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "location": "docs/md_v2/advanced/session-management-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
      "location": "docs/md_v2/extraction/css-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
      "location": "docs/md_v2/advanced/magic-mode.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
        "traceability_granularity": "Method",
        "trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "location": "docs/md_v2/extraction/chunking.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ChunkingStrategy",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
        "type": "Class",
        "relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy"
      },
      {
        "title": "SlidingWindowChunking",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class SlidingWindowChunking(ChunkingStrategy): def __init__(self, window_size=100, step=50, **kwargs): \"\"\" Initialize the sliding window chunking strategy with the given window size and step size. Args: window_size (int): The size of the sliding window in words. step (int): The step size for sliding the window in words. \"\"\" self.window_size = window_size self.step = step def chunk(self, text: str) -> list: words = text.split() chunks = [] if len(words) <= self.window_size: return [text] for i in range(0, len(words) - self.window_size + 1, self.step): chunk = ' '.join(words[i:i + self.window_size]) chunks.append(chunk) # Handle the last chunk if it doesn't align perfectly if i + self.window_size < len(words): chunks.append(' '.join(words[-self.window_size:])) return chunks",
        "type": "Class",
        "relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking"
      }
    ]
  },
  {
    "document": {
      "text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.links",
        "location": "crawl4ai/models.py",
        "content": "links: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.links"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
      "location": "docs/md_v2/advanced/magic-mode.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
        "traceability_granularity": "Method",
        "trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
        "traceability_granularity": "Class",
        "trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "location": "docs/md_v2/advanced/session-management.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the core methods needed to implement the session management behaviors described in the documentation, including crawl() for executing page actions and hook management for handling session state.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "location": "docs/md_v2/basic/simple-crawling.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.success"
      },
      {
        "title": "CrawlResult.error_message",
        "location": "crawl4ai/models.py",
        "content": "error_message: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.error_message"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.status_code",
        "location": "crawl4ai/models.py",
        "content": "status_code: Optional[int] = None",
        "type": "Class Attribute",
        "relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.status_code"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "cosine.md -> CrawlResult.success"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
        "traceability_granularity": "Method",
        "trace_chain": "cosine.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "cosine.md -> CrawlResult.extracted_content"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-selection.md -> CrawlResult.extracted_content"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-selection.md -> CrawlResult.media"
      }
    ]
  },
  {
    "document": {
      "text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.cleaned_html",
        "location": "crawl4ai/models.py",
        "content": "cleaned_html: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.cleaned_html"
      }
    ]
  },
  {
    "document": {
      "text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.html",
        "location": "crawl4ai/models.py",
        "content": "html: str",
        "type": "Class Attribute",
        "relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.html"
      },
      {
        "title": "CrawlResult.fit_markdown",
        "location": "crawl4ai/models.py",
        "content": "fit_markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.fit_markdown"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.cleaned_html",
        "location": "crawl4ai/models.py",
        "content": "cleaned_html: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.cleaned_html"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "ChunkingStrategy",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
        "type": "Class",
        "relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy"
      },
      {
        "title": "RegexChunking",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class RegexChunking(ChunkingStrategy): def __init__(self, patterns=None, **kwargs): if patterns is None: patterns = [r'\n\n'] # Default split pattern self.patterns = patterns def chunk(self, text: str) -> list: paragraphs = [text] for pattern in self.patterns: new_paragraphs = [] for paragraph in paragraphs: new_paragraphs.extend(re.split(pattern, paragraph)) paragraphs = new_paragraphs return paragraphs",
        "type": "Class",
        "relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "quickstart.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "location": "docs/md_v2/extraction/chunking.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "NlpSentenceChunking",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class NlpSentenceChunking(ChunkingStrategy): def __init__(self, **kwargs): load_nltk_punkt() pass def chunk(self, text: str) -> list: # Improved regex for sentence splitting # sentence_endings = re.compile( # r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z][A-Z]\\.)(?<![A-Za-z]\\.)(?<=\\.|\\?|\\!|\n)\\s' # ) # sentences = sentence_endings.split(text) # sens = [sent.strip() for sent in sentences if sent] from nltk.tokenize import sent_tokenize sentences = sent_tokenize(text) sens = [sent.strip() for sent in sentences] return list(set(sens))",
        "type": "Class",
        "relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking"
      },
      {
        "title": "ChunkingStrategy",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
        "type": "Class",
        "relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "location": "docs/md_v2/extraction/chunking.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ChunkingStrategy",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
        "type": "Class",
        "relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy"
      },
      {
        "title": "FixedLengthWordChunking",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class FixedLengthWordChunking(ChunkingStrategy): def __init__(self, chunk_size=100, **kwargs): \"\"\" Initialize the fixed-length word chunking strategy with the given chunk size. Args: chunk_size (int): The size of each chunk in words. \"\"\" self.chunk_size = chunk_size def chunk(self, text: str) -> list: words = text.split() return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]",
        "type": "Class",
        "relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking"
      }
    ]
  },
  {
    "document": {
      "text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
      "location": "docs/md_v2/extraction/css.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "css.md -> CrawlResult.success"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> CrawlResult"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "location": "docs/md_v2/extraction/chunking.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ChunkingStrategy",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
        "type": "Class",
        "relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy"
      },
      {
        "title": "RegexChunking",
        "location": "crawl4ai/chunking_strategy.py",
        "content": "class RegexChunking(ChunkingStrategy): def __init__(self, patterns=None, **kwargs): if patterns is None: patterns = [r'\n\n'] # Default split pattern self.patterns = patterns def chunk(self, text: str) -> list: paragraphs = [text] for pattern in self.patterns: new_paragraphs = [] for paragraph in paragraphs: new_paragraphs.extend(re.split(pattern, paragraph)) paragraphs = new_paragraphs return paragraphs",
        "type": "Class",
        "relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
        "traceability_granularity": "Class",
        "trace_chain": "chunking.md -> ChunkingStrategy -> RegexChunking"
      }
    ]
  },
  {
    "document": {
      "text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "location": "docs/md_v2/advanced/session-management-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")",
        "type": "Method",
        "relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.metadata",
        "location": "crawl4ai/models.py",
        "content": "metadata: Optional[dict] = None",
        "type": "Class Attribute",
        "relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.metadata"
      }
    ]
  },
  {
    "document": {
      "text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-selection.md -> CrawlResult.media"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.html",
        "location": "crawl4ai/models.py",
        "content": "html: str",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.html"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "location": "docs/md_v2/extraction/llm.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
        "traceability_granularity": "Method",
        "trace_chain": "llm.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> CrawlResult"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "llm.md -> CrawlResult.extracted_content"
      }
    ]
  },
  {
    "document": {
      "text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "location": "docs/md_v2/advanced/session-management.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "session-management.md -> CrawlResult.success"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "session-management.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "location": "docs/md_v2/extraction/css-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "css-advanced.md -> CrawlResult.success"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
        "traceability_granularity": "Method",
        "trace_chain": "css-advanced.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "css-advanced.md -> CrawlResult.extracted_content"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "cosine.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
        "traceability_granularity": "Method",
        "trace_chain": "cosine.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "location": "docs/md_v2/index.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "index.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "index.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "index.md -> CrawlResult.markdown"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
        "traceability_granularity": "Class",
        "trace_chain": "index.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
        "traceability_granularity": "Class",
        "trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
        "traceability_granularity": "Method",
        "trace_chain": "index.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
      "location": "docs/md_v2/extraction/css-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "location": "docs/md_v2/basic/page-interaction.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.cleaned_html",
        "location": "crawl4ai/models.py",
        "content": "cleaned_html: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "page-interaction.md -> CrawlResult.cleaned_html"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
      "location": "docs/md_v2/extraction/css-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
      "location": "docs/md_v2/extraction/css-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
        "traceability_granularity": "Method",
        "trace_chain": "overview.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> CrawlResult"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncWebCrawler"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
      "location": "docs/md_v2/advanced/proxy-security.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
        "traceability_granularity": "Method",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "browser-config.md -> CrawlResult.success"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult.screenshot",
        "location": "crawl4ai/models.py",
        "content": "screenshot: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "browser-config.md -> CrawlResult.screenshot"
      }
    ]
  },
  {
    "document": {
      "text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "location": "docs/md_v2/advanced/session-management-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
        "traceability_granularity": "Class",
        "trace_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
        "type": "Method",
        "relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
        "traceability_granularity": "Method",
        "trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()"
      }
    ]
  },
  {
    "document": {
      "text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "location": "docs/md_v2/basic/simple-crawling.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
        "traceability_granularity": "Method",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.fit_markdown",
        "location": "crawl4ai/models.py",
        "content": "fit_markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.fit_markdown"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.media"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.links",
        "location": "crawl4ai/models.py",
        "content": "links: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.links"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "location": "docs/md_v2/extraction/llm.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
        "traceability_granularity": "Method",
        "trace_chain": "llm.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncWebCrawler"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "llm.md -> CrawlResult.extracted_content"
      }
    ]
  },
  {
    "document": {
      "text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
      "location": "docs/md_v2/extraction/overview.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
        "traceability_granularity": "Class",
        "trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/advanced/proxy-security.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
        "traceability_granularity": "Method",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "cosine.md -> CrawlResult.success"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
        "traceability_granularity": "Method",
        "trace_chain": "cosine.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.error_message",
        "location": "crawl4ai/models.py",
        "content": "error_message: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "cosine.md -> CrawlResult.error_message"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> CrawlResult"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "cosine.md -> CrawlResult.extracted_content"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.cleaned_html",
        "location": "crawl4ai/models.py",
        "content": "cleaned_html: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.cleaned_html"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "location": "docs/md_v2/extraction/llm.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
        "traceability_granularity": "Method",
        "trace_chain": "llm.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "llm.md -> CrawlResult.extracted_content"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
        "traceability_granularity": "Class",
        "trace_chain": "llm.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.extracted_content"
      },
      {
        "title": "LLMExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
        "type": "Class",
        "relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.fit_markdown",
        "location": "crawl4ai/models.py",
        "content": "fit_markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.fit_markdown"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
      "location": "docs/md_v2/basic/page-interaction.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult.fit_markdown",
        "location": "crawl4ai/models.py",
        "content": "fit_markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.fit_markdown"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "content-processing.md -> CrawlResult.media"
      }
    ]
  },
  {
    "document": {
      "text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
      "location": "docs/md_v2/advanced/content-processing.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
        "traceability_granularity": "Method",
        "trace_chain": "content-processing.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
        "traceability_granularity": "Class",
        "trace_chain": "content-processing.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/basic/simple-crawling.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
        "traceability_granularity": "Method",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
      "location": "docs/md_v2/extraction/css-advanced.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
        "traceability_granularity": "Class",
        "trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
      "location": "docs/md_v2/extraction/css.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> AsyncWebCrawler"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
        "traceability_granularity": "Class",
        "trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "location": "docs/md_v2/advanced/proxy-security.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
        "traceability_granularity": "Method",
        "trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
        "traceability_granularity": "Class",
        "trace_chain": "proxy-security.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "location": "docs/md_v2/basic/simple-crawling.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "CrawlResult.success",
        "location": "crawl4ai/models.py",
        "content": "success: bool",
        "type": "Class Attribute",
        "relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.success"
      },
      {
        "title": "CrawlResult.media",
        "location": "crawl4ai/models.py",
        "content": "media: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.media"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
        "traceability_granularity": "Method",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult.error_message",
        "location": "crawl4ai/models.py",
        "content": "error_message: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.error_message"
      },
      {
        "title": "CrawlResult.links",
        "location": "crawl4ai/models.py",
        "content": "links: Dict[str, List[Dict]] = {}",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.links"
      },
      {
        "title": "CrawlResult.markdown",
        "location": "crawl4ai/models.py",
        "content": "markdown: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "simple-crawling.md -> CrawlResult.markdown"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
        "traceability_granularity": "Class",
        "trace_chain": "simple-crawling.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
      "location": "docs/md_v2/basic/browser-config.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
        "traceability_granularity": "Method",
        "trace_chain": "browser-config.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
        "traceability_granularity": "Class",
        "trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
      "location": "docs/md_v2/basic/quickstart.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
        "traceability_granularity": "Method",
        "trace_chain": "quickstart.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
        "traceability_granularity": "Class",
        "trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      }
    ]
  },
  {
    "document": {
      "text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "location": "docs/md_v2/basic/content-selection.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
        "traceability_granularity": "Method",
        "trace_chain": "content-selection.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> CrawlResult"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "content-selection.md -> AsyncWebCrawler"
      }
    ]
  },
  {
    "document": {
      "text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "location": "docs/md_v2/basic/output-formats.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "JsonCssExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
        "type": "Class",
        "relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncWebCrawler"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
        "traceability_granularity": "Method",
        "trace_chain": "output-formats.md -> AsyncWebCrawler.arun()"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "CrawlResult.extracted_content",
        "location": "crawl4ai/models.py",
        "content": "extracted_content: Optional[str] = None",
        "type": "Class Attribute",
        "relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
        "traceability_granularity": "Statement-level",
        "trace_chain": "output-formats.md -> CrawlResult.extracted_content"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
        "traceability_granularity": "Class",
        "trace_chain": "output-formats.md -> CrawlResult"
      }
    ]
  },
  {
    "document": {
      "text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
      "location": "docs/md_v2/basic/page-interaction.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "AsyncCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
        "type": "Class",
        "relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
        "type": "Class",
        "relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler"
      },
      {
        "title": "CrawlResult",
        "location": "crawl4ai/models.py",
        "content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
        "type": "Class",
        "relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> CrawlResult"
      },
      {
        "title": "AsyncPlaywrightCrawlerStrategy",
        "location": "crawl4ai/async_crawler_strategy.py",
        "content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
        "type": "Class",
        "relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
        "traceability_granularity": "Class",
        "trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy"
      },
      {
        "title": "AsyncWebCrawler.arun()",
        "location": "crawl4ai/async_webcrawler.py",
        "content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
        "type": "Method",
        "relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
        "traceability_granularity": "Method",
        "trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()"
      }
    ]
  },
  {
    "document": {
      "text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
      "location": "docs/md_v2/extraction/cosine.md",
      "type": ""
    },
    "artifacts": [
      {
        "title": "ExtractionStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
        "type": "Class",
        "relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy"
      },
      {
        "title": "CosineStrategy",
        "location": "crawl4ai/extraction_strategy.py",
        "content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
        "type": "Class",
        "relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
        "traceability_granularity": "Class",
        "trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy"
      }
    ]
  }
]